{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWttnt7I/RLbaeSMd5Ayub",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ihrisikesa/DWS/blob/main/DWS_Sahabat_Sehat_Progress_Lobar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- installs (first run only) ---\n",
        "!pip -q install gspread gspread_dataframe google-api-python-client google-auth-httplib2 google-auth-oauthlib pandas==2.2.2\n",
        "\n",
        "# --- auth ---\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import pandas as pd\n",
        "import google.auth\n",
        "from googleapiclient.discovery import build\n",
        "import gspread\n",
        "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
        "\n",
        "# --- credentials and services ---\n",
        "creds, _ = google.auth.default()\n",
        "SCOPES = [\n",
        "    \"https://www.googleapis.com/auth/drive\",\n",
        "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
        "    \"https://www.googleapis.com/auth/documents\",\n",
        "]\n",
        "creds = creds.with_scopes(SCOPES)\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "drive = build(\"drive\", \"v3\", credentials=creds)\n",
        "docs  = build(\"docs\",  \"v1\", credentials=creds)"
      ],
      "metadata": {
        "id": "IJH9V8gx2gu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp_kOUUOx01H"
      },
      "outputs": [],
      "source": [
        "# ==== 0) Setup & imports ====\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # skip if already mounted\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# ==== 1) Config ====\n",
        "FOLDER = Path('/content/drive/MyDrive/sahabat_sehat_lobar/dws/DWS export (File responses)/File (File responses)')\n",
        "FILE_PATTERN = '*.csv'   # adjust if needed\n",
        "THRESH_MIN = 3.788                              # threshold in minutes\n",
        "FILTER_KUNJUNGAN_SUCCESS = False                # set True to keep only Kunjungan Rumah + Success\n",
        "\n",
        "# ==== 2) Read latest matching CSV from folder ====\n",
        "matches = list(FOLDER.glob(FILE_PATTERN))\n",
        "if not matches:\n",
        "    raise FileNotFoundError(f\"No matching CSV found in: {FOLDER} with pattern {FILE_PATTERN}\")\n",
        "\n",
        "latest = max(matches, key=lambda p: p.stat().st_mtime)\n",
        "print(f\"Reading: {latest.name}\")\n",
        "\n",
        "# Let pandas infer delimiter robustly (comma/semicolon)\n",
        "df = pd.read_csv(latest, encoding='utf-8-sig', sep=None, engine='python')\n",
        "\n",
        "# ==== 3) Find key columns robustly ====\n",
        "def find_col(candidates_regex, columns):\n",
        "    \"\"\"\n",
        "    Return the first column whose name matches any regex in candidates_regex (case-insensitive).\n",
        "    \"\"\"\n",
        "    for pat in candidates_regex:\n",
        "        for c in columns:\n",
        "            if re.search(pat, str(c), flags=re.I):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "manual_col = find_col(\n",
        "    [r'^manual\\s*check[- ]?in$', r'manual\\s*check[- ]?in'], df.columns\n",
        ")\n",
        "if manual_col is None:\n",
        "    raise KeyError(\"Couldn't find a 'Manual check-in' column. \"\n",
        "                   f\"Available columns: {list(df.columns)}\")\n",
        "\n",
        "date_col = find_col(\n",
        "    [r'^date$', r'^tanggal$', r'^tgl$'], df.columns\n",
        ")\n",
        "if date_col is None:\n",
        "    raise KeyError(\"Couldn't find a Date/Tanggal column. \"\n",
        "                   \"Expected one of: Date, Tanggal, tgl (case-insensitive).\")\n",
        "\n",
        "print(f\"Using columns -> Manual check-in: '{manual_col}', Date: '{date_col}'\")\n",
        "\n",
        "# ==== 4) Parse 'Manual check-in' durations to minutes ====\n",
        "def minutes_from_manual(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Parse durations like:\n",
        "      '2 h 10 m', '15 m', '900 s', '1h', '1h30m', '75m 30s', '12:30', or plain '15'\n",
        "    Returns minutes as float.\n",
        "    \"\"\"\n",
        "    s = series.astype(\"string\")\n",
        "\n",
        "    def _to_min(x) -> float:\n",
        "        if x is None or pd.isna(x):\n",
        "            return np.nan\n",
        "        x = str(x).strip().lower()\n",
        "        if not x:\n",
        "            return np.nan\n",
        "\n",
        "        # capture number+unit pairs (h/m/s), with or without spaces (e.g., '1h30m', '2 h 10 m')\n",
        "        pairs = re.findall(r'(\\d+(?:\\.\\d+)?)\\s*([hms])', x)\n",
        "        if pairs:\n",
        "            total = 0.0\n",
        "            for val, unit in pairs:\n",
        "                v = float(val)\n",
        "                if unit == 'h':\n",
        "                    total += v * 60\n",
        "                elif unit == 'm':\n",
        "                    total += v\n",
        "                elif unit == 's':\n",
        "                    total += v / 60.0\n",
        "            return total\n",
        "\n",
        "        # handle mm:ss format\n",
        "        if re.fullmatch(r'\\d+:\\d{1,2}', x):\n",
        "            mm, ss = x.split(':')\n",
        "            return float(mm) + float(ss) / 60.0\n",
        "\n",
        "        # plain number -> assume minutes\n",
        "        if re.fullmatch(r'\\d+(?:\\.\\d+)?', x):\n",
        "            return float(x)\n",
        "\n",
        "        return np.nan\n",
        "\n",
        "    return s.map(_to_min).astype(float)\n",
        "\n",
        "df[\"dur_ci_co_min\"] = minutes_from_manual(df[manual_col])\n",
        "\n",
        "\n",
        "# ==== 5) Optional filter to Kunjungan Rumah + (Success OR Scheduled) ====\n",
        "if FILTER_KUNJUNGAN_SUCCESS:\n",
        "    df_kunj = df.loc[\n",
        "        df[\"Title\"].eq(\"Kunjungan Rumah\")\n",
        "        & df[\"Status\"].isin([\"Success\"])\n",
        "    ].copy()\n",
        "else:\n",
        "    df_kunj = df.copy()\n",
        "\n",
        "\n",
        "# ==== 6) Build Date key (date only) ====\n",
        "df_kunj[\"Date_key\"] = pd.to_datetime(df_kunj[date_col], errors=\"coerce\").dt.date\n",
        "\n",
        "# ==== 7) Indicators vs 3.788 minutes ====\n",
        "mins = pd.to_numeric(df_kunj[\"dur_ci_co_min\"], errors=\"coerce\")\n",
        "df_kunj[\"lt_3_788\"] = (mins < THRESH_MIN).astype(\"Int8\")\n",
        "df_kunj[\"gt_3_788\"] = (mins > THRESH_MIN).astype(\"Int8\")\n",
        "\n",
        "# ==== 8) Group & aggregate ====\n",
        "# Group by Worker × Date × Title × Status; keep NaNs (dropna=False)\n",
        "gb = (df_kunj.groupby([\"Worker\", \"Date_key\", \"Title\", \"Status\"], dropna=False, as_index=False)\n",
        "      .agg(\n",
        "          total_m      = (\"dur_ci_co_min\", lambda s: pd.to_numeric(s, errors=\"coerce\").fillna(0).sum()),\n",
        "          n_rows       = (\"dur_ci_co_min\", \"size\"),\n",
        "          n_nonnull    = (\"dur_ci_co_min\", lambda s: pd.to_numeric(s, errors=\"coerce\").notna().count()),\n",
        "          lt_3_788_sum = (\"lt_3_788\", \"sum\"),\n",
        "          gt_3_788_sum = (\"gt_3_788\", \"sum\"),\n",
        "      ))\n",
        "\n",
        "# Cast numeric columns safely\n",
        "for c in [\"total_m\", \"n_rows\", \"n_nonnull\", \"lt_3_788_sum\", \"gt_3_788_sum\"]:\n",
        "    gb[c] = pd.to_numeric(gb[c], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
        "\n",
        "# Averages & percentages (use n_nonnull to avoid bias from missing durations)\n",
        "gb[\"avg_m\"] = np.where(gb[\"n_nonnull\"] > 0, gb[\"total_m\"] / gb[\"n_nonnull\"], np.nan).round(2)\n",
        "gb[\"percentage_<3788\"] = np.where(gb[\"n_nonnull\"] > 0, gb[\"lt_3_788_sum\"] / gb[\"n_nonnull\"] * 100, np.nan).round(2)\n",
        "gb[\"percentage_>3788\"] = np.where(gb[\"n_nonnull\"] > 0, gb[\"gt_3_788_sum\"] / gb[\"n_nonnull\"] * 100, np.nan).round(2)\n",
        "\n",
        "# ==== 9) Tidy & rename ====\n",
        "gb = (gb.rename(columns={\"Date_key\":\"Date\"})\n",
        "        .sort_values([\"Worker\",\"Date\",\"Title\",\"Status\"])\n",
        "        .rename(columns={\n",
        "            \"total_m\": \"total_duration_minutes\",\n",
        "            \"n_rows\":  \"n_visit\",\n",
        "            \"avg_m\":   \"avg_duration_minutes\",\n",
        "            \"lt_3_788_sum\": \"<3788_sum\",\n",
        "            \"gt_3_788_sum\": \">3788_sum\"\n",
        "        }))\n",
        "\n",
        "# Optional: keep only Kunjungan Rumah + Success in the final table (independent of earlier toggle)\n",
        "gb_kunj = gb[(gb[\"Title\"] == \"Kunjungan Rumah\") & (gb[\"Status\"].isin([\"Success\"]))].copy()\n",
        "\n",
        "# ==== 10) Peek results ====\n",
        "print(\"\\n=== Aggregated (all titles/status) head ===\")\n",
        "display(gb.head(10))\n",
        "\n",
        "print(\"\\n=== Kunjungan Rumah + Success head ===\")\n",
        "display(gb_kunj.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wide table of status counts per Worker × Date × Title\n",
        "status_wide = (gb[[\"Worker\", \"Date\", \"Title\", \"Status\", \"n_visit\"]]\n",
        "               .assign(Status=lambda d: d[\"Status\"].astype(\"string\").str.strip())   # optional normalize\n",
        "               .pivot_table(index=[\"Worker\", \"Date\", \"Title\"],\n",
        "                            columns=\"Status\",\n",
        "                            values=\"n_visit\",\n",
        "                            aggfunc=\"sum\",\n",
        "                            fill_value=0)\n",
        "               .reset_index())\n",
        "\n",
        "# Drop the columns axis name (\"Status\")\n",
        "status_wide.columns.name = None\n",
        "\n",
        "# Ensure these columns exist even if absent in data\n",
        "for c in [\"Success\", \"Scheduled\", \"Issues\", \"Started\"]:\n",
        "    if c not in status_wide.columns:\n",
        "        status_wide[c] = 0\n",
        "\n",
        "# Total across the four statuses\n",
        "status_cols = [\"Success\", \"Scheduled\", \"Issues\", \"Started\"]\n",
        "status_wide[status_cols] = status_wide[status_cols].astype(\"int64\")\n",
        "status_wide[\"total_jobs\"] = status_wide[status_cols].sum(axis=1).astype(\"int64\")\n",
        "\n",
        "# % Success per total jobs (0–100). Safe for zero totals.\n",
        "status_wide[\"pct_success\"] = np.where(\n",
        "    status_wide[\"total_jobs\"] > 0,\n",
        "    status_wide[\"Success\"] / status_wide[\"total_jobs\"] * 100,\n",
        "    np.nan\n",
        ").round(2)\n",
        "\n",
        "# (optional) pretty text like \"83.33%\"\n",
        "status_wide[\"pct_success_str\"] = status_wide[\"pct_success\"].map(\n",
        "    lambda x: f\"{x:.2f}%\" if pd.notna(x) else \"\"\n",
        ")\n",
        "\n",
        "# Reorder and (optionally) filter to Kunjungan Rumah\n",
        "status_wide = status_wide[[\"Worker\", \"Date\", \"Title\"] + status_cols + [\"total_jobs\", \"pct_success\", \"pct_success_str\"]]\n",
        "status_wide = status_wide.query(\"Title == 'Kunjungan Rumah'\").copy()\n",
        "\n",
        "status_wide.head(10)\n",
        "status_wide.columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtP1cUCB0Lo6",
        "outputId": "4361572a-67b5-44e6-c90e-e078268a70cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Worker', 'Date', 'Title', 'Success', 'Scheduled', 'Issues', 'Started',\n",
              "       'total_jobs', 'pct_success', 'pct_success_str'],\n",
              "      dtype='string')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume you already have:\n",
        "# gb_kunj  (from your grouped Kunjungan Rumah table)\n",
        "# status_wide (wide counts with Success/Scheduled/Issues/Started/total_jobs/pct_success)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Make sure the join keys have the same types/format\n",
        "gb_kunj[\"Worker\"] = gb_kunj[\"Worker\"].astype(\"string\").str.strip()\n",
        "status_wide[\"Worker\"] = status_wide[\"Worker\"].astype(\"string\").str.strip()\n",
        "\n",
        "gb_kunj[\"Title\"] = gb_kunj[\"Title\"].astype(\"string\").str.strip()\n",
        "status_wide[\"Title\"] = status_wide[\"Title\"].astype(\"string\").str.strip()\n",
        "\n",
        "gb_kunj[\"Date\"] = pd.to_datetime(gb_kunj[\"Date\"], errors=\"coerce\").dt.date\n",
        "status_wide[\"Date\"] = pd.to_datetime(status_wide[\"Date\"], errors=\"coerce\").dt.date\n",
        "\n",
        "# 2) (Optional) sanity check: status_wide should be unique on the keys\n",
        "# If not unique, uncomment to compress to one row\n",
        "# if status_wide.duplicated([\"Worker\",\"Date\",\"Title\"]).any():\n",
        "#     status_cols = [\"Success\",\"Scheduled\",\"Issues\",\"Started\",\"total_jobs\"]\n",
        "#     status_wide = (status_wide\n",
        "#                    .groupby([\"Worker\",\"Date\",\"Title\"], as_index=False)[status_cols].sum())\n",
        "\n",
        "# 3) Join (left join to keep all gb_kunj rows)\n",
        "final_tbl = gb_kunj.merge(\n",
        "    status_wide,\n",
        "    on=[\"Worker\", \"Date\", \"Title\"],\n",
        "    how=\"left\",\n",
        "    validate=\"m:1\"   # many gb_kunj rows to 1 status_wide row\n",
        ")\n",
        "\n",
        "# 4) Clean up missing values & types for the added status columns\n",
        "for c in [\"Success\", \"Scheduled\", \"Issues\", \"Started\", \"total_jobs\"]:\n",
        "    if c in final_tbl.columns:\n",
        "        final_tbl[c] = pd.to_numeric(final_tbl[c], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
        "\n",
        "if \"pct_success\" in final_tbl.columns:\n",
        "    final_tbl[\"pct_success\"] = pd.to_numeric(final_tbl[\"pct_success\"], errors=\"coerce\").round(2)\n",
        "\n",
        "if \"pct_success_str\" in final_tbl.columns:\n",
        "    final_tbl[\"pct_success_str\"] = final_tbl[\"pct_success_str\"].fillna(\"\")\n",
        "\n",
        "# 5) Optional: reorder columns for readability\n",
        "order = [\n",
        "    \"Worker\",\"Date\",\"Title\",\"Status\",\n",
        "    \"total_duration_minutes\",\"n_visit\",\"n_nonnull\",\"<3788_sum\",\">3788_sum\",\n",
        "    \"avg_duration_minutes\",\"percentage_<3788\",\"percentage_>3788\",\n",
        "    \"Success\",\"Scheduled\",\"Issues\",\"Started\",\"total_jobs\",\"pct_success\",\"pct_success_str\"\n",
        "]\n",
        "final_tbl = final_tbl[[c for c in order if c in final_tbl.columns] +\n",
        "                      [c for c in final_tbl.columns if c not in order]]\n",
        "\n",
        "# Preview\n",
        "final_tbl.head(10)\n",
        "final_tbl.columns"
      ],
      "metadata": {
        "id": "poIx0R5P2ml3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gspread gspread_dataframe\n",
        "#from google.colab import auth\n",
        "#auth.authenticate_user()\n",
        "\n"
      ],
      "metadata": {
        "id": "_3np-N9o3h8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7d183f6"
      },
      "source": [
        "import pandas as pd\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "from google.auth import default\n",
        "\n",
        "creds, project = default()\n",
        "gc = gspread.Client(auth=creds)\n",
        "\n",
        "sh = gc.open_by_key(\"1pInt5qeyZuvvqYuEm7ckhF7iIxnAGkXvdZIAWriHCJk\")\n",
        "\n",
        "try:\n",
        "    ws = sh.worksheet(\"Summary\")\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    # First time: create + write everything (with header)\n",
        "    ws = sh.add_worksheet(title=\"Summary\",\n",
        "                          rows=str(len(final_tbl)+10),\n",
        "                          cols=str(len(final_tbl.columns)+5))\n",
        "    set_with_dataframe(ws, final_tbl, include_index=False,\n",
        "                       include_column_header=True, resize=True)\n",
        "    print(\"Created 'Summary' and wrote initial data.\")\n",
        "else:\n",
        "    # Read existing (use row 1 as header). Drop empty rows/cols.\n",
        "    existing = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
        "    existing = (existing.dropna(how=\"all\")\n",
        "                        .loc[:, ~existing.columns.str.contains(r\"^Unnamed\")])\n",
        "\n",
        "    # Choose what to append:\n",
        "    df_to_append = final_tbl.copy()\n",
        "\n",
        "    # OPTIONAL: append only rows with a new date (adjust column name if needed)\n",
        "    for date_col in [\"Date\", \"Tanggal\", \"date\", \"tgl\"]:\n",
        "        if date_col in existing.columns and date_col in final_tbl.columns:\n",
        "            ex_dates  = pd.to_datetime(existing[date_col], errors=\"coerce\").dt.date\n",
        "            new_dates = pd.to_datetime(final_tbl[date_col], errors=\"coerce\").dt.date\n",
        "            df_to_append = final_tbl[~new_dates.isin(set(ex_dates.dropna()))].copy()\n",
        "            break\n",
        "\n",
        "    if df_to_append.empty:\n",
        "        print(\"Nothing new to append.\")\n",
        "    else:\n",
        "        # First empty row (after header + data)\n",
        "        start_row = len(ws.get_all_values()) + 1\n",
        "\n",
        "        # Ensure the sheet has enough rows\n",
        "        needed = start_row + len(df_to_append) - 1\n",
        "        if needed > ws.row_count:\n",
        "            ws.add_rows(needed - ws.row_count)\n",
        "\n",
        "        # Append WITHOUT header and WITHOUT resizing\n",
        "        set_with_dataframe(ws, df_to_append,\n",
        "                           row=start_row, col=1,\n",
        "                           include_index=False,\n",
        "                           include_column_header=False,\n",
        "                           resize=False)\n",
        "        print(f\"Appended {len(df_to_append)} rows to 'Summary'.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# AUTH (single source of truth) – keep once\n",
        "# =======================\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import google.auth\n",
        "from googleapiclient.discovery import build\n",
        "import gspread\n",
        "from google.auth import default\n",
        "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Scopes + creds\n",
        "SCOPES = [\n",
        "    \"https://www.googleapis.com/auth/drive\",\n",
        "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
        "    \"https://www.googleapis.com/auth/documents\",\n",
        "]\n",
        "creds, _ = default()\n",
        "creds = creds.with_scopes(SCOPES)\n",
        "\n",
        "# Clients (rename Drive service to avoid collision with colab drive)\n",
        "gc         = gspread.authorize(creds)\n",
        "drive_svc  = build(\"drive\", \"v3\", credentials=creds)\n",
        "docs_svc   = build(\"docs\",  \"v1\", credentials=creds)\n",
        "\n",
        "# If you need to mount the Colab Drive filesystem:\n",
        "from google.colab import drive as gdrive\n",
        "gdrive.mount('/content/drive')\n",
        "\n",
        "# =======================\n",
        "# CONFIG\n",
        "# =======================\n",
        "SHEET_ID          = \"1pInt5qeyZuvvqYuEm7ckhF7iIxnAGkXvdZIAWriHCJk\"\n",
        "SHEET_TAB         = \"Summary\"\n",
        "TEMPLATE_DOC_ID   = \"1W-r9DzptEhUWpikAnhU9YDxc_QHcaQAYUKe-Y77fAcY\"\n",
        "DEST_FOLDER_ID    = None     # e.g. \"your_folder_id\"\n",
        "OUTPUT_TAB        = \"kartu_evaluasi_links\"\n",
        "SKIP_IF_HAS_LINK  = True     # set to False to force regeneration for testing\n",
        "\n",
        "# =======================\n",
        "# Helpers\n",
        "# =======================\n",
        "MONTH_ID = [\"Januari\",\"Februari\",\"Maret\",\"April\",\"Mei\",\"Juni\",\n",
        "            \"Juli\",\"Agustus\",\"September\",\"Oktober\",\"November\",\"Desember\"]\n",
        "\n",
        "def tanggal_id(dt: pd.Timestamp) -> str:\n",
        "    dt = pd.to_datetime(dt, errors=\"coerce\")\n",
        "    if pd.isna(dt):\n",
        "        return \"\"\n",
        "    return f\"{dt.day} {MONTH_ID[dt.month-1]} {dt.year}\"\n",
        "\n",
        "def copy_template(new_name: str) -> str:\n",
        "    body = {\"name\": new_name}\n",
        "    if DEST_FOLDER_ID:\n",
        "        body[\"parents\"] = [DEST_FOLDER_ID]\n",
        "    file = drive_svc.files().copy(fileId=TEMPLATE_DOC_ID, body=body).execute()\n",
        "    return file[\"id\"]\n",
        "\n",
        "def replace_placeholders(doc_id: str, mapping: dict):\n",
        "    requests = [{\"replaceAllText\": {\n",
        "        \"containsText\": {\"text\": f\"{{{{{k}}}}}\", \"matchCase\": True},\n",
        "        \"replaceText\": str(v) if v is not None else \"\"\n",
        "    }} for k, v in mapping.items()]\n",
        "    docs_svc.documents().batchUpdate(documentId=doc_id, body={\"requests\": requests}).execute()\n",
        "\n",
        "# Quick permission smoke test – will raise if template not accessible\n",
        "_ = docs_svc.documents().get(documentId=TEMPLATE_DOC_ID).execute()\n",
        "print(\"Template is accessible.\")\n",
        "\n",
        "# =======================\n",
        "# CONFIG\n",
        "# =======================\n",
        "SUCCESS_PCT_THRESHOLD = 60.0  # only create cards when %Success < 40\n",
        "\n",
        "# =======================\n",
        "# Read the source Sheet (keep all columns for metrics)\n",
        "# =======================\n",
        "ws = gc.open_by_key(SHEET_ID).worksheet(SHEET_TAB)\n",
        "df_full = get_as_dataframe(ws, evaluate_formulas=True).dropna(how=\"all\")\n",
        "\n",
        "# normalize types\n",
        "df_full[\"Worker\"] = df_full.get(\"Worker\").astype(\"string\").str.strip()\n",
        "df_full[\"Title\"]  = df_full.get(\"Title\").astype(\"string\").str.strip()\n",
        "df_full[\"Date\"]   = pd.to_datetime(df_full.get(\"Date\"), errors=\"coerce\")\n",
        "\n",
        "# numeric metrics\n",
        "for col in [\"Success\", \"total_jobs\"]:\n",
        "    if col in df_full.columns:\n",
        "        df_full[col] = pd.to_numeric(df_full[col], errors=\"coerce\")\n",
        "\n",
        "# -----------------------\n",
        "# Build metrics per Worker × Title × Date\n",
        "# -----------------------\n",
        "metrics = (df_full.dropna(subset=[\"Worker\",\"Title\",\"Date\"])\n",
        "                   .assign(Date=df_full[\"Date\"].dt.date)\n",
        "                   .groupby([\"Worker\",\"Title\",\"Date\"], as_index=False)\n",
        "                   .agg(Success=(\"Success\",\"sum\"),\n",
        "                        total_jobs=(\"total_jobs\",\"sum\")))\n",
        "\n",
        "metrics[\"pct_success_calc\"] = np.where(\n",
        "    metrics[\"total_jobs\"] > 0,\n",
        "    metrics[\"Success\"] / metrics[\"total_jobs\"] * 100.0,\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# Filter to below-threshold combos\n",
        "# -----------------------\n",
        "to_generate = metrics.loc[\n",
        "    (metrics[\"total_jobs\"] > 0) & (metrics[\"pct_success_calc\"] < SUCCESS_PCT_THRESHOLD)\n",
        "].copy()\n",
        "\n",
        "# Existing links (for skip)\n",
        "try:\n",
        "    ws_links = gc.open_by_key(SHEET_ID).worksheet(OUTPUT_TAB)\n",
        "    existing_links = get_as_dataframe(ws_links).dropna(how=\"all\")\n",
        "except Exception:\n",
        "    ws_links = None\n",
        "    existing_links = pd.DataFrame(columns=[\"Worker\",\"Title\",\"Date\",\"doc_url\"])\n",
        "\n",
        "if SKIP_IF_HAS_LINK and not existing_links.empty:\n",
        "    ex = (existing_links[[\"Worker\",\"Title\",\"Date\"]]\n",
        "          .assign(Date=pd.to_datetime(existing_links[\"Date\"], errors=\"coerce\").dt.date))\n",
        "    to_generate = (to_generate.merge(ex, on=[\"Worker\",\"Title\",\"Date\"], how=\"left\", indicator=True)\n",
        "                             .loc[lambda d: d[\"_merge\"].eq(\"left_only\"),\n",
        "                                  [\"Worker\",\"Title\",\"Date\",\"Success\",\"total_jobs\",\"pct_success_calc\"]])\n",
        "\n",
        "print(\"Rows to generate (< 40% success):\", len(to_generate))\n",
        "print(to_generate.head(10))\n",
        "\n",
        "# =======================\n",
        "# Create Docs for the filtered rows\n",
        "# =======================\n",
        "created = []\n",
        "for r in to_generate.itertuples(index=False):\n",
        "    worker, title, d, succ, tot, pct = r.Worker, r.Title, pd.to_datetime(r.Date), int(r.Success or 0), int(r.total_jobs or 0), float(r.pct_success_calc)\n",
        "\n",
        "    nice_date = tanggal_id(d)\n",
        "    doc_name  = f\"Kartu Evaluasi - {worker} - {title} - {nice_date}\"\n",
        "\n",
        "    new_doc_id = copy_template(doc_name)\n",
        "    temuan_str = f\"{succ}/{tot} ({pct:.2f}%)\"  # what goes into {{TEMUAN}}\n",
        "\n",
        "    replace_placeholders(new_doc_id, {\n",
        "        \"NAMA\": worker,\n",
        "        \"JENIS_KUNJUNGAN\": title,\n",
        "        \"TANGGAL_KUNJUNGAN\": nice_date,\n",
        "        \"TEMUAN\": temuan_str,\n",
        "        # optional extras if you added these placeholders:\n",
        "        \"SUKSES\": succ,\n",
        "        \"TOTAL_JOBS\": tot,\n",
        "        \"PCT_SUKSES\": f\"{pct:.2f}%\"\n",
        "    })\n",
        "\n",
        "    created.append({\n",
        "        \"Worker\": worker,\n",
        "        \"Title\":  title,\n",
        "        \"Date\":   d.date(),\n",
        "        \"doc_url\": f\"https://docs.google.com/document/d/{new_doc_id}/edit\"\n",
        "    })\n",
        "\n",
        "links_df = pd.DataFrame(created)\n",
        "print(f\"Created {len(links_df)} documents (< {SUCCESS_PCT_THRESHOLD}% success).\")\n",
        "\n",
        "# =======================\n",
        "# Write/append links tab\n",
        "# =======================\n",
        "sh = gc.open_by_key(SHEET_ID)\n",
        "if ws_links is None:\n",
        "    ws_links = sh.add_worksheet(title=OUTPUT_TAB, rows=200, cols=6)\n",
        "    set_with_dataframe(ws_links, links_df, include_index=False, resize=True)\n",
        "else:\n",
        "    existing = get_as_dataframe(ws_links).dropna(how=\"all\")\n",
        "    out = pd.concat([existing, links_df], ignore_index=True)\n",
        "    ws_links.clear()\n",
        "    set_with_dataframe(ws_links, out, include_index=False, resize=True)\n",
        "\n",
        "print(\"Links updated:\", OUTPUT_TAB)\n",
        "\n"
      ],
      "metadata": {
        "id": "Aw49lKTt2bJA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}